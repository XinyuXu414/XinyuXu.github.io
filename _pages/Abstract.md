<span class='anchor' id='abstract'></span>

# High-Accuracy Phase Unwrapping Based on Binarized Wrap Count
Spatial phase unwrapping is essential for converting wrapped phase fringes into a continuous unwrapped phase map, which is critical for various high-precision measurement technologies. The accuracy of phase unwrapping directly affects measurement precision. Recently, deep learning-based phase unwrapping has emerged as a promising alternative to traditional methods, primarily due to its strong resilience against noise. However, existing approaches often struggle to produce consistent results, limiting their practical applicability. This study introduces Binarized Wrap Count Phase Unwrapping (BWCPU), a novel method that utilizes neural networks to analyze phase gradient structures through binarized wrap counts. This approach reduces prediction complexity while ensuring accurate phase segmentation. In structured light surface measurements, BWCPU significantly decreases misinterpretations in noisy conditions, achieving a remarkable 76.9% improvement over leading deep learning-based wrap-count estimation methods. Furthermore, by employing a stitching algorithm known as unidirectional optimal seam stitching, BWCPU extends its capabilities to handle 1024×1024 patterns, showcasing its potential for high-precision measurements in noisy environments.

# Motion-induced phase shift for dynamic structured light measurement
Structured light 3D shape measurement is extensively utilized in semiconductor inspection, smart manufacturing, and biomedical imaging due to its rapid measurement speed, high precision, and versatile applicability to different objects. However, the traditional implementations of this method often require that the object remains static while recording the phase-shifting structured light images, which limits the adaptability of dynamic measurement. Here, we propose a dynamic 3D shape measurement using structured light based on motion-induced phase shift (MIPS). As the object moves, the surface features distort the fringe pattern, resulting in a phase shifting effect. By employing the MIPS method, we can determine the phase even in the situations where the knowledge of phase shifting conditions is not accurate. This enables the acquisition of the 3D topography of the object surface with a high level of precision. Experimental results demonstrate that the MIPS method can accurately measure the 3D shape of objects moving as fast as 100 mm/s, with a relative discrepancy of less than 0.23%.

# High frame-rate reconfigurable diffractive neural network based on superpixels
The existing implementations of reconfigurable diffractive neural network rely on both a liquid crystal spatial light modulator and a digital micromirror device, which results in the complexity in the alignment of optical system and constrained computational speed. Here, we propose a superpixel diffractive neural network that leverages solely a digital micromirror device to control neuron bias and connection. This approach considerably simplifies the optical system and achieves an computational speed of 326Hz per neural layer. We validate our method through experiments in digit classification, achieving an accuracy of 82.6%, and action recognition, attaining a perfect accuracy of 100%. Our findings demonstrate the effectiveness of the superpixel diffractive neural network in simplifying the optical system and enhancing computational speed, opening new possibilities for real-time optical information processing applications.

# Large field of view Shack-Hartmann wavefront sensor based on high-density lens transfer function retrieval
The Shack-Hartmann wavefront sensor (SHWS) is known for its high accuracy and robust wavefront sensing capabilities. However, conventional compact SHWS confronts limitations in measuring field-of-view to meet emerging applications’ increasing demands. Here, we propose a high-density lens transfer function retrieval (HDLTR)-based SHWS to expand its field-of-view. In HDLTR-SHWS, an additional lens is introduced into the measurement system, which converges input wavefront with large aperture onto detectable aperture of sensor. A densely sampling set of phase delays is employed to retrieve the transfer function of the lens and to isolate lens distortion. Then, an accurate error compensation model is established to suppress errors introduced by the lens distortion, which is used to accurately demodulate the convergent wavefront. We also utilize a global spot matching method to reconstruct the converged wavefront with a large dynamic range. Our experimental results demonstrate that the HDLTR-SHWS expands the field-of-view of SHWS by a factor of 24.9 and achieves an accuracy of less than 1/80λ.

# PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation
Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various deep semantic segmentation networks have been proposed for pixel-level classification and segmentation tasks. However, the imaging angles are often arbitrary in real world, such as water body images in remote sensing, and capillary and polyp images in medical field, and we usually cannot obtain prior orientation information to guide these networks to extract more effective features. Additionally, learning the features of objects with multiple orientation information is also challenging, as most CNN-based semantic segmentation networks do not have rotation equivariance to resist the disturbance from orientation information. To address the same, in this paper, we first establish a universal convolution-group framework to more fully utilize the orientation information and make the networks rotation equivariant. Then, we  mathematically construct the padding-based rotation equivariant convolution mode (PreCM), which can be used not only for multi-scale images and convolution kernels, but also as a replacement component to replace multiple convolutions, like dilated convolution, transposed convolution, variable stride convolution. In order to verify the realization of rotation equivariance, a new evaluation metric named rotation difference (RD) is finally proposed. The experiments carried out on the datesets Satellite Images of Water Bodies, DRIVE and Floodnet show that the PreCM-based networks can achieve better segmentation performance than the original and data augmentation-based networks. In terms of the average RD value, the former is 0\% and the latter two are respectively 7.0503% and 3.2606%

# Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective of Manifold Distillation
Point cloud is often regarded as a discrete sampling of Riemannian manifold and plays a pivotal role in the 3D image interpretation. Particularly, rotation perturbation, an unexpected small change in rotation caused by various factors (like equipment offset, system instability, measurement errors and so on), can easily lead to the inferior results in point cloud learning tasks. However, classical point cloud learning methods are sensitive to rotation perturbation, and the existing networks with rotation robustness also have much room for improvements in terms of performance and noise tolerance. Given these, this paper remodels the point cloud from the perspective of manifold as well as designs a manifold distillation method to achieve the robustness of rotation perturbation without any coordinate transformation. In brief, during the training phase, we introduce a teacher network to learn the rotation robustness information and transfer this information to the student network through online distillation. In the inference phase, the student network directly utilizes the original 3D coordinate information to achieve the robustness of rotation perturbation. Experiments carried out on four different datasets verify the effectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN classification datasets with random rotation perturbations, our classification accuracy has respectively improved by 4.92% and 4.41%, compared to popular rotation-robust networks; on the ShapeNet and S3DIS segmentation datasets, compared to the rotation-robust networks, the improvements of mIoU are 7.36% and 4.82%, respectively. Besides, from the experimental results, the proposed algorithm also shows excellent performance in resisting noise and outliers. 
